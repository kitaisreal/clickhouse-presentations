<!DOCTYPE html>
<html lang="en">
<head>
    <title>Power of Small Optimizations</title>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="shower/themes/yandex/styles/screen-16x9.css">

    <style type="text/css">
        code { display: block; white-space: pre; background-color: #EEE; }
   </style>
</head>
<body class="shower list">
    <header class="caption">
        <h1>Power of Small Optimizations</h1>
    </header>

    <section class="slide" id="cover">
        <h1 style="margin-top: 150px">Power of Small Optimizations</h1>
    </section>

    <section class="slide">
        <h2>About me</h2>

        <p>Maksim, database management systems developer.</p>
    </section>

    <section class="slide">
        <h2>Stories</h2>

        <p>1. Concurrency.</p>
        <p>2. Low-level optimizations.</p>
        <p>3. Algorithms.</p>
    </section>

    <section class="slide">
        <h1 style="margin-top: 150px;">Concurrency</h1>
    </section>

    <section class="slide">
        <h2>CPU Underutilization</h2>

        <p>CPU underutilization in one of our biggest clusters.</p>
        <img style="width: 100%;" src="pictures/context_lock_cpu_average_1.jpg"/>
    </section>

    <section class="slide">
        <h2>ContextLockWait</h2>

        <p>After 1 year during similar indicent we spot that <b>ContextLockWait</b> async profile event periodically increased ...</p>
        <img style="width: 100%; height: 60%;" src="pictures/context_lock_events.jpg"/>
    </section>

    <section class="slide">
        <h2>Stack Traces</h2>

        <p>During indident we periodically dumped all stack traces to understand if all threads are blocked on lock inside Context using <b>system.stack_trace</b>.</p>

        <code style="font-size:10pt;">WITH arrayMap(x -> demangle(addressToSymbol(x)), trace) AS all
SELECT thread_name, thread_id, query_id, arrayStringConcat(all, '\n') AS res
FROM <b>system.stack_trace</b> LIMIT 1 FORMAT Vertical;
        </code>

        <code style="font-size:10pt;">Row 1:
──────
thread_name: clickhouse-serv
thread_id:   125441
query_id:
res:         pthread_cond_wait
std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&)
BaseDaemon::waitForTerminationRequest()
DB::Server::main(/*arguments*/)
Poco::Util::Application::run()
DB::Server::run()
Poco::Util::ServerApplication::run(int, char**)
mainEntryClickHouseServer(int, char**)
main
__libc_start_main
_start
        </code>
    </section>

    <section class="slide">
        <h2>Profile Events</h2>

        <p>Per query profile events:</p>

        <code style="font-size:10pt;">...

M(<b>GlobalThreadPoolJobs</b>,
  "Counts the number of jobs that have been pushed to the global thread pool.",
  ValueType::Number) \

M(<b>GlobalThreadPoolLockWaitMicroseconds</b>,
  "Total time threads have spent waiting for locks in the global thread pool.",
  ValueType::Microseconds) \

M(<b>GlobalThreadPoolJobWaitTimeMicroseconds</b>,
  "Measures the elapsed time from when a job is scheduled in the thread pool to when it is picked up
  for execution by a worker thread. This metric helps identify delays in job processing, indicating
  the responsiveness of the thread pool to new tasks.",
  ValueType::Microseconds) \

M(<b>LocalThreadPoolLockWaitMicroseconds</b>,
  "Total time threads have spent waiting for locks in the local thread pools.",
   ValueType::Microseconds) \

...
        </code>
    </section>

    <section class="slide">
        <h2>ContextLockWaitMicroseconds</h2>

        <p>We added <b>ContextLockWaitMicroseconds</b> event to <b>ProfileEvents</b>.</p>

        <code style="font-size:10pt;">...

M(ContextLock,
    "Number of times the lock of Context was acquired or tried to acquire. This is global lock.",
    ValueType::Number) \

M(<b>ContextLockWaitMicroseconds</b>,
    "Context lock wait time in microseconds",
    ValueType::Microseconds) \

...
        </code>

        <br>
        <p><a href="https://github.com/ClickHouse/ClickHouse/pull/55029">https://github.com/ClickHouse/ClickHouse/pull/55029</a></p>
    </section>

    <section class="slide">
        <h2>Benchmark ContextLock wait time</h2>

        <p style="margin-bottom: 8px;">Example query:</p>
        <code style="font-size:10pt;">SELECT UserID, count(*) FROM (SELECT * FROM hits_clickbench LIMIT 10) GROUP BY UserID
0 rows in set. Elapsed: 0.005 sec.</code>
        <p style="font-size: 8pt;"></p>
        <p style="margin-bottom: 8px;">Run benchmark:</p>
<code style="font-size:10pt;">clickhouse-benchmark --query="SELECT UserID, count(*) FROM (SELECT * FROM hits_clickbench LIMIT 10)
GROUP BY UserID" --concurrency=200</code>
        <p style="font-size: 8pt;"></p>
        <p style="margin-bottom: 8px;">Check results:</p>
<code style="font-size:10pt;">SELECT quantileExact(0.5)(lock_wait_milliseconds), max(lock_wait_milliseconds) FROM
(
    SELECT (ProfileEvents['ContextLockWaitMicroseconds'] / 1000.0) AS lock_wait_milliseconds
    FROM system.query_log WHERE lock_wait_milliseconds > 0
)

┌─<b>quantileExact(0.5)(lock_wait_milliseconds)</b>─┬─<b>max(lock_wait_milliseconds)</b>──┐
│                                     <b>17.452</b> │                      <b>382.326</b> │
└────────────────────────────────────────────┴──────────────────────────────┘</code>
    </section>

    <section class="slide">
        <h2>Concurrency</h2>

        <img style="width: 100%;" src="pictures/context_lock_context_architecture.png"/>
    </section>

    <section class="slide">
        <h2>Concurrency</h2>

        <p><b>ContextSharedPart</b> is responsible for storing and providing access to global shared objects that are shared between all sessions and queries, for example: Thread pools,
            Server paths, Global trackers, Clusters information.</p>

        <p><b>Context</b> is responsible for storing and providing access to query or session-specific objects, for example: query settings, query caches, query current database.</p>
    </section>

    <section class="slide">
        <h2>Concurrency</h2>

        <p>During query execution, ClickHouse can create a lot of Contexts because each subquery in ClickHouse can have unique settings. For example:</p>
        <code style="font-size:10pt;">SELECT id, value
FROM (
    SELECT id, value
    FROM test_table
    SETTINGS max_threads = 16
)
WHERE id > 10
SETTINGS max_threads = 32
        </code>

        <p>A large number of low-latency, concurrent queries with many subqueries will create a lot of <b>Contexts</b> per query, and the problem becomes even bigger.</p>
    </section>

    <section class="slide">
        <h2>Concurrency</h2>

        <img style="width: 100%;" src="pictures/context_lock_context_architecture.png"/>
    </section>

    <section class="slide">
        <h2>Concurrency</h2>

        <p>The problem was that a single mutex was used for most of the synchronization between <b>Context</b> and <b>ContextSharedPart</b>, even when we worked with objects local to <b>Context</b>.<p>

    </section>

    <section class="slide">
        <h2>Concurrency</h2>

        <p>We did a big refactoring, replacing a single global mutex with two read-write mutexes. One global read-write mutex for <b>ContextSharedPart</b>
            and one local read-write mutex for each <b>Context</b>.</p>
        <p>We used read-write mutexes because most of the time we do a lot
            of concurrent reads (for example read settings or some path) and rarely concurrent writes.</p>
        <p>In many places, we completely got rid of synchronization where it was used for initialization and used <b>call_once</b> for objects that are initialized only once.</p>
        <p><a href="https://github.com/ClickHouse/ClickHouse/pull/55121">https://github.com/ClickHouse/ClickHouse/pull/55121</a></p>
    </section>

    <section class="slide">
        <h2>Concurrency</h2>

        <img style="width: 100%;" src="pictures/context_lock_context_architecture_after_refactoring.png"/>
    </section>

    <section class="slide">
        <h2>Concurrency</h2>

        <p><b>ContextSharedPart</b> and <b>Context</b> both contain a lot of fields and it is very hard to properly split synchronization between them manually.</p>
        <p>We added clang Thread Safety Analysis annotations to all fields <a href="https://clang.llvm.org/docs/ThreadSafetyAnalysis.html">https://clang.llvm.org/docs/ThreadSafetyAnalysis.html</a></p>

        <code style="font-size:14pt;">clang -c -Wthread-safety example.cpp</code>
        <p style="font-size: 10pt;"></p>
        <p><a href="https://github.com/ClickHouse/ClickHouse/pull/55278">https://github.com/ClickHouse/ClickHouse/pull/55278</a></p>
    </section>

    <section class="slide">
        <h2>TSA example from documentation</h2>

        <code style="font-size:10pt;">class BankAccount {
private:
    <b>Mutex mu;</b>
    int   balance <b>GUARDED_BY(mu)</b>;

    void depositImpl(int amount) <b>/* TO FIX: REQUIRES(mu) */</b> {
        balance += amount;       // WARNING! Cannot write balance without locking mu.
    }

    void withdrawImpl(int amount) <b>REQUIRES(mu)</b> {
        balance -= amount;       // OK. Caller must have locked mu.
    }

public:
    void withdraw(int amount) {
        mu.Lock();
        withdrawImpl(amount);    // OK.  We've locked mu.
        <b>/* TO FIX: mu.unlock() or use std::lock_guard */</b>
    }                          // WARNING!  Failed to unlock mu.

    void transferFrom(BankAccount& b, int amount) {
        mu.Lock();
        <b>/* TO FIX: lock() and unlock() b.mu */</b>
        b.withdrawImpl(amount);  // WARNING!  Calling withdrawImpl() requires locking b.mu.
        depositImpl(amount);     // OK.  depositImpl() has no requirements.
        mu.Unlock();
    }
};
        </code>
    </section>

    <section class="slide">
        <h2>TSA capabilities from documentation</h2>

        <p>Thread safety analysis provides a way of protecting resources with capabilities.</p>
        <p>A resource is either a data member, or a function/method that provides access to some underlying resource.</p>
        <p>The analysis ensures that the calling thread cannot access the resource (i.e. call the function, or read/write the data) unless it has the capability to do so.</p>
    </section>

    <section class="slide">
        <h2>TSA capabilities from documentation</h2>

        <p>A thread may hold a capability either exclusively or shared. An exclusive capability can be held by only one thread at a time,
            while a shared capability can be held by many threads at the same time.</p>
        <p>This mechanism enforces a multiple-reader, single-writer pattern.
            Write operations to protected data require exclusive access, while read operations require only shared access.</p>
    </section>

    <section class="slide">
        <h2>TSA capabilities from documentation</h2>

        <p>Capabilities are associated with named C++ objects which declare specific methods to acquire and release the capability. The name of
            the object serves to identify the capability.</p>
        <p>The most common example is a mutex. For example, if <b>mu</b> is a mutex,
            then calling <b>mu.Lock()</b> causes the calling thread to acquire the capability to access data that is protected by <b>mu</b>.
            Similarly, calling <b>mu.Unlock()</b> releases that capability.</p>

        <code style="font-size:10pt;">Mutex mu1, mu2;
int a GUARDED_BY(mu1);
int b GUARDED_BY(mu2);

void foo() REQUIRES(mu1, mu2) {
    a = 0;
    b = 0;
}</code>
    </section>

    <section class="slide">
        <h2>TSA annotations</h2>

        <p>For implementation of capability classes and functions: <b>CAPABILITY(...)</b>, <b>SCOPED_CAPABILITY</b>, <b>ACQUIRE(…)</b>,
            <b>ACQUIRE_SHARED(…)</b>, <b>RELEASE(…)</b>, <b>RELEASE_SHARED(…)</b>, <b>RELEASE_GENERIC(…)</b></p>
        <p>For protecting data: <b>GUARDED_BY(...)</b>, <b>PT_GUARDED_BY(...)</b>, <b>REQUIRES(…)</b>, <b>REQUIRES_SHARED(…)</b></p>
        <p>Utility: <b>NO_THREAD_SAFETY_ANALYSIS</b></p>
    </section>

    <section class="slide">
        <h2>TSA annotations</h2>

        <p>In LLVM standard library all mutex implementations are annotated with TSA annotations. Example <b>std::mutex</b>:</p>
        <code style="font-size:10pt;">class _LIBCPP_TYPE_VIS <b>_LIBCPP_THREAD_SAFETY_ANNOTATION(capability("mutex"))</b> mutex
{
    __libcpp_mutex_t __m_ = _LIBCPP_MUTEX_INITIALIZER;

public:
    _LIBCPP_INLINE_VISIBILITY
    _LIBCPP_CONSTEXPR mutex() = default;

    mutex(const mutex&) = delete;
    mutex& operator=(const mutex&) = delete;

#if defined(_LIBCPP_HAS_TRIVIAL_MUTEX_DESTRUCTION)
    ~mutex() = default;
#else
    ~mutex() _NOEXCEPT;
#endif

    <b>void lock() _LIBCPP_THREAD_SAFETY_ANNOTATION(acquire_capability())</b>;
    <b>bool try_lock() _NOEXCEPT _LIBCPP_THREAD_SAFETY_ANNOTATION(try_acquire_capability(true))</b>;
    <b>void unlock() _NOEXCEPT _LIBCPP_THREAD_SAFETY_ANNOTATION(release_capability())</b>;

    typedef __libcpp_mutex_t* native_handle_type;
    _LIBCPP_INLINE_VISIBILITY native_handle_type native_handle() {return &__m_;}
};
        </code>
    </section>

    <section class="slide">
        <h2>TSA annotations problems</h2>

        <p>Default implementation of <b>std::shared_mutex</b> is slow. In ClickHouse we have our own implementation.</p>
        <p>In ClickHouse we want to have mutexes with additional logic during lock/unlock. For example update metrics.</p>
        <p>In both cases we do not want to have a lot of duplicated TSA annotations in all of our mutexes. We want to hide them and to have generic solution.</p>
        <p>We designed our own <b>SharedMutexHelper</b> template class using <a href="https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern">CRTP pattern</a> that implements <b>SharedMutex</b>
            requirementes <a href="https://en.cppreference.com/w/cpp/named_req/SharedMutex">https://en.cppreference.com/w/cpp/named_req/SharedMutex</a> and adds TSA annotations.</p>
    </section>

    <section class="slide">
        <h2>SharedMutexHelper</h2>

        <code style="font-size:10pt;">template &lt;typename Derived, typename MutexType = SharedMutex&gt;
class <b>TSA_CAPABILITY("SharedMutexHelper")</b> SharedMutexHelper
{
    auto & getDerived() { return static_cast&lt;Derived &&gt;(*this); }

public:
    // Exclusive ownership
    void <b>lock() TSA_ACQUIRE()</b> { getDerived().lockImpl(); }

    bool <b>try_lock() TSA_TRY_ACQUIRE(true)</b> { getDerived().tryLockImpl(); }

    void <b>unlock() TSA_RELEASE()</b> { getDerived().unlockImpl(); }

    // Shared ownership
    void <b>lock_shared() TSA_ACQUIRE_SHARED()</b> { getDerived().lockSharedImpl(); }

    bool <b>try_lock_shared() TSA_TRY_ACQUIRE_SHARED(true)</b> { getDerived().tryLockSharedImpl(); }

    void <b>unlock_shared() TSA_RELEASE_SHARED()</b> { getDerived().unlockSharedImpl(); }

protected:
    /// Default implementations for all *Impl methods.
    void lockImpl() TSA_NO_THREAD_SAFETY_ANALYSIS { mutex.lock(); }

    ...

    void unlockSharedImpl() TSA_NO_THREAD_SAFETY_ANALYSIS { mutex.unlock_shared(); }

    MutexType mutex;
};</code>
    </section>

    <section class="slide">
        <h2>ContextSharedMutex</h2>

        <code style="font-size:10pt;">class ContextSharedMutex : public SharedMutexHelper&lt;ContextSharedMutex&gt;
{
private:
    using Base = SharedMutexHelper&lt;ContextSharedMutex, SharedMutex&gt;;
    friend class SharedMutexHelper&lt;ContextSharedMutex, SharedMutex&gt;;

    void <b>lockImpl</b>()
    {
        ProfileEvents::increment(ProfileEvents::ContextLock);
        CurrentMetrics::Increment increment{CurrentMetrics::ContextLockWait};
        Stopwatch watch;
        Base::lockImpl();
        ProfileEvents::increment(ProfileEvents::ContextLockWaitMicroseconds,
            watch.elapsedMicroseconds());
    }

    void <b>lockSharedImpl</b>()
    {
        ProfileEvents::increment(ProfileEvents::ContextLock);
        CurrentMetrics::Increment increment{CurrentMetrics::ContextLockWait};
        Stopwatch watch;
        Base::lockSharedImpl();
        ProfileEvents::increment(ProfileEvents::ContextLockWaitMicroseconds,
            watch.elapsedMicroseconds());
    }
};</code>
    </section>

    <section class="slide">
        <h2>SharedLockGuard</h2>

        <p>In LLVM standard library <b>std::shared_lock</b> did not use TSA annotations.</p>
        <p>We implemented our own <b>SharedLockGuard</b>:</p>

        <code style="font-size:10pt;">template &lt;typename Mutex&gt;
class <b>TSA_SCOPED_LOCKABLE</b> SharedLockGuard
{
public:
    explicit SharedLockGuard(Mutex & mutex_) <b>TSA_ACQUIRE_SHARED(mutex_)</b>
        : mutex(mutex_) { mutex_.lock_shared(); }

    ~SharedLockGuard() <b>TSA_RELEASE()</b> { mutex.unlock_shared(); }

private:
    Mutex & mutex;
};</code>
    </section>

    <section class="slide">
        <h2>TSA Examples</h2>

        <code style="font-size:10pt;">struct ContextSharedPart : boost::noncopyable
{
    /// For access of most of shared objects.
    <b>mutable ContextSharedMutex mutex;</b>

    /// Path to the data directory, with a slash at the end.
    String path <b>TSA_GUARDED_BY(mutex)</b>;

    /// Path to the directory with some control flags for server maintenance.
    String flags_path <b>TSA_GUARDED_BY(mutex)</b>;

    /// Path to the directory with user provided files, usable by 'file' table function.
    String dictionaries_lib_path <b>TSA_GUARDED_BY(mutex)</b>;

    /// Path to the directory with user provided scripts.
    String user_scripts_path <b>TSA_GUARDED_BY(mutex)</b>;

    /// Path to the directory with filesystem caches.
    String filesystem_caches_path <b>TSA_GUARDED_BY(mutex)</b>;

    /// Path to the directory with user provided filesystem caches.
    String filesystem_cache_user_path <b>TSA_GUARDED_BY(mutex)</b>;

    /// Global configuration settings.
    ConfigurationPtr config <b>TSA_GUARDED_BY(mutex)</b>;
};</code>
    </section>

    <section class="slide">
        <h2>TSA Examples</h2>

        <code style="font-size:10pt;">String Context::getPath() const
{
    SharedLockGuard lock(shared->mutex);
    return shared->path;
}

String Context::getFlagsPath() const
{
    SharedLockGuard lock(shared->mutex);
    return shared->flags_path;
}

String Context::getUserFilesPath() const
{
    SharedLockGuard lock(shared->mutex);
    return shared->user_files_path;
}

String Context::getDictionariesLibPath() const
{
    SharedLockGuard lock(shared->mutex);
    return shared->dictionaries_lib_path;
}
</code>
    </section>

    <section class="slide">
        <h2>Performance Improvements</h2>

        <p>Benchmark:</p>
        <code style="font-size: 12pt;">clickhouse benchmark -r --ignore-error \
--concurrency=500 \
--timelimit 600 \
--connect_timeout=20 &lt; queries.txt</code>

        <p style="font-size: 10pt;"></p>

        <p>Results:<br>Before ~200 QPS. After ~600 QPS (<b>~3x better</b>).<br>
            Before CPU utilization of only ~20%. After ~60% (<b>~3x better</b>).<br>
            Before median query time 1s. After ~0.6s (<b>~2x better</b>).<br>
            Before slowest queries took ~75s. After ~6s (<b>~12x better</b>).
        </p>
    </section>

    <section class="slide">
        <h2>Performance Improvements</h2>

        <p>We also were able to fully utilize ClickHouse instance with <b>--concurrency=1000</b>.</p>

        <p>Results:<br>
            ~1,000 QPS<br>
            ~95-96% CPU utilization<br>
        </p>
    </section>

    <section class="slide">
        <h1 style="margin-top: 150px;">Low-level optimizations</h1>
    </section>

    <section class="slide">
        <h2>Low-level optimizations</h2>

        <p>In December 2023, during the development of some ClickHouse features, when I ran some queries that read a lot of String columns,</p>
        <p>I noticed in the <b>perf-top</b> and flame graphs that we can spend around 20-40% of query execution time on strings deserialization.</p>
        <p>I knew that string deserialization place was already heavily optimized in ClickHouse, but I decided to dig deeper.</p>
    </section>

    <section class="slide">
        <h2>Low-level optimizations</h2>

        <p>In <b>perf-top</b>, I saw something like this:</p>

        <code style="font-size: 10pt;">Samples: 1M of event 'cycles', 4000 Hz, Event count (approx.): 756969039682 lost: 0/0 drop: 0/17041
Overhead  Shared Object                   Symbol
  39.00%  clickhouse                      <b>[.] DB::deserializeBinarySSE2&lt;1&gt;</b>
  15.12%  clickhouse                      [.] DB::PODArrayBase&lt;1ul, 4096ul&gt;::resize&lt;&gt;
  13.57%  clickhouse                      [.] DB::PODArrayDetails::byte_size
   9.36%  clickhouse                      [.] LZ4::(anonymous namespace)::decompressImpl&lt;16ul, true&gt;
   4.36%  [kernel]                        [k] copy_user_generic_string
   2.60%  clickhouse                      [.] DB::FunctionStringOrArrayToT::executeImpl
   2.55%  clickhouse                      [.] CityHash_v1_0_2::CityHash128WithSeed
   2.31%  clickhouse                      [.] LZ4::(anonymous namespace)::decompressImpl&lt;16ul, false&gt;
   1.40%  clickhouse                      [.] memcpy
   1.15%  clickhouse                      [.] DB::findExtremeImplAVX2
   0.52%  [kernel]                        [k] filemap_get_read_batch
   0.52%  clickhouse                      [.] LZ4::(anonymous namespace)::decompressImpl&lt;8ul, true&gt;
   0.40%  clickhouse                      [.] LZ4::(anonymous namespace)::decompressImpl&lt;32ul, false&gt;
        </code>
    </section>


    <section class="slide">
        <h2>Low-level optimizations</h2>

        <p>Most of the time is spent in <b>DB::deserializeBinarySSE2</b>, and it is expected. What is not expected that we see
        <b>PODArray::resize</b> and <b>PODArray::byte_size</b> methods. If you check the <b>DB::deserializeBinarySSE2</b> assembly,
        you can notice that the <b>PODArray::resize</b> function is called from it, and that function call is not inlined.</p>

        <code style="font-size: 10pt;">...

0.32 │    │  inc    %r13
1.25 │    │  mov    %r13,(%rax)
5.19 │    │  add    $0x8,%rax
0.02 │    │  mov    %rax,0x8(%rbp)
0.23 │    │  mov    0x30(%rsp),%r12
0.36 │    │  mov    %r12,%rdi
0.14 │    │  mov    %r13,%rsi
2.67 │    │→ <b>callq  DB::PODArrayBase&lt;1ul, 4096ul, Allocator&lt;false, false&gt;, 63ul, 64ul&gt;::resize&lt;&gt;</b>
2.95 │    │  mov    0x28(%rsp),%rdx
1.70 │    │  test   %rdx,%rdx
1.51 │    │↑ je     51
0.00 │    │  lea    0x11(%r15),%rax

...</code>
    </section>


    <section class="slide">
        <h2>Low-level optimizations</h2>

        <p>If we check the <b>PODArray::resize</b> assembly, we will notice it calls <b>PODArray::byte_size</b> function
            and we can also see that <b>PODArray::resize</b> function call overhead is high.</p>

        <code style="font-size: 10pt;">...

 0.10 │104:   mov   $0x1,%esi
 0.71 │       mov   %r14,%rdi
 5.92 │     → <b>callq DB::PODArrayDetails::byte_size</b>
 5.63 │       add   %r12,%rax
 0.10 │       mov   %rax,0x8(%rbx)
30.30 │       pop   %rbx
 0.76 │       pop   %r12
 1.96 │       pop   %r13
 4.42 │       pop   %r14
 2.89 │       pop   %r15
11.24 │     ← retq

...
        </code>
    </section>

    <section class="slide">
        <h2>Low-level optimizations</h2>

        <p>In C++ code <b>deserializeBinarySSE2</b> function looked like this:</p>

<code style="font-size: 8pt;">void deserializeBinarySSE2(ColumnString::Chars & data, ColumnString::Offsets & offsets, ReadBuffer & istr, size_t limit)
{
    size_t offset = data.size();
    for (size_t i = 0; i &lt; limit; ++i)
    {
        if (istr.eof())
            break;

        UInt64 size;
        readVarUInt(size, istr);
        ...

        <b>data.resize(offset);</b>

        if (size)
        {
#ifdef __SSE2__
            /// An optimistic branch in which more efficient copying is possible.
            if (offset + 16 * UNROLL_TIMES &lt;= data.capacity() &&
                istr.position() + size + 16 * UNROLL_TIMES &lt;= istr.buffer().end())
            {
                ...
            }
            else
#endif
            {
                istr.readStrict(reinterpret_cast&lt;char*&gt;(&data[offset - size - 1]), size);
            }
        }

        data[offset - 1] = 0;
    }
}</code>
    </section>

    <section class="slide">
        <h2>Low-level optimizations</h2>

        <p>In this specific function, we work with <b>PODArray</b> as just a characters buffer, and we can manually control
            the resize process and resize buffer with some constant resize factor, for example, 2.</p>
        <p>We also use the <b>resize_exact</b> function to reduce memory allocation size.
            So inside the deserialization loop, we replace:</p>

<code style="font-size: 12pt;">data.resize(offset);</code>

with:

<code style="font-size: 12pt;">if (unlikely(offset &gt; data.size()))
    data.resize_exact(roundUpToPowerOfTwoOrZero(std::max(offset, data.size() * 2)));</code>
    </section>

    <section class="slide">
        <h2>Performance improvements</h2>

        <p>As a result, we have such performance improvement for a query that I used for the optimization test. Around <b>20%</b> improvement.</p>

        <p>Before:</p>
<code style="font-size: 12pt;">SELECT max(length(value)) FROM test_table FORMAT Null

0 rows in set. Elapsed: 0.855 sec. Processed 1.50 billion rows, 19.89 GB
(1.75 billion rows/s., 23.27 GB/s.)
Peak memory usage: 1.24 MiB.</code>

        <p style="font-size: 10pt;"></p>
        <p>After:</p>
<code style="font-size: 12pt;">SELECT max(length(value)) FROM test_table FORMAT Null

0 rows in set. Elapsed: 0.691 sec. Processed 1.50 billion rows, 19.89 GB
(2.17 billion rows/s., 28.79 GB/s.)
Peak memory usage: 1.17 MiB.</code>
    </section>

    <section class="slide">
        <h2>Performance improvements</h2>

        <p>Results of performance tests from ClickHouse CI:</p>

<table style="font-size: 10pt;">
    <thead>
        <tr>
            <th>Query</th>
            <th>Old (s)</th>
            <th>New (s)</th>
            <th>Ratio of speedup(-) or slowdown(+)</th>
            <th>Relative difference (new - old) / old</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>SELECT count() FROM empty_strings WHERE NOT ignore(s)</td>
            <td>0.449</td>
            <td>0.27</td>
            <td>-1.662x</td>
            <td><b>-0.399</b></td>
        </tr>
        <tr>
            <td>select anyHeavy(OpenstatSourceID) from hits_100m_single where OpenstatSourceID != '' group by intHash32(UserID) % 1000000 FORMAT Null</td>
            <td>0.088</td>
            <td>0.066</td>
            <td>-1.328x</td>
            <td><b>-0.247</b></td>
        </tr>
        <tr>
            <td>select anyHeavy(OpenstatCampaignID) from hits_100m_single where OpenstatCampaignID != '' group by intHash32(UserID) % 1000000 FORMAT Null</td>
            <td>0.088</td>
            <td>0.066</td>
            <td>-1.32x</td>
            <td><b>-0.243</b></td>
        </tr>
        <tr>
            <td>SELECT count() FROM hits_100m_single WHERE NOT ignore(format('{}Hello{}', MobilePhoneModel, PageCharset))</td>
            <td>0.321</td>
            <td>0.28</td>
            <td>-1.147x</td>
            <td><b>-0.129</b></td>
        </tr>
        <tr>
            <td>SELECT str FROM test_full_10 FORMAT Null</td>
            <td>0.074</td>
            <td>0.058</td>
            <td>-1.282x</td>
            <td><b>-0.22</b></td>
        </tr>
        <tr>
            <td>SELECT count() FROM hits_100m_single WHERE NOT ignore(substring(PageCharset, 1, 2))</td>
            <td>0.135</td>
            <td>0.116</td>
            <td>-1.16x</td>
            <td><b>-0.138</b></td>
        </tr>
    </tbody>
</table>
    </section>

    <section class="slide">
        <h2>Performance improvements</h2>

        <p>As a result, this optimization improved performance for queries that spend a lot of execution time on string deserialization by <b>10-20%</b> on average.</p>
        <p>For some queries, even for <b>60%</b>.</p>

        <p><a href="https://github.com/ClickHouse/ClickHouse/pull/57717">https://github.com/ClickHouse/ClickHouse/pull/57717</a></p>
    </section>


    <section class="slide">
        <h1 style="margin-top: 150px;">Algorithms</h1>
    </section>

    <section class="slide">
        <h2>Abstractions and Algorithms</h2>

        <p>There is no silver bullet, or best algorithm for any task.</p>
        <p>Try to choose the fastest possible algorithm/algorithms for <b>your specific task</b>.</p>
        <p>Performance must be evaluated on real data.</p>
        <p>Most of the algorithms are affected by data distribution.</p>
    </section>

    <section class="slide">
        <h2>Sorting</h2>

        <p>Each problem can have a lot degrees of freedom. For example Sorting:</p>

        <p>Stable / nonstable?</p>
        <p>External / in RAM?</p>
        <p>With limit/without limit?</p>
        <p>Is the data already almost sorted?</p>
        <p>What about data distribution? How many unique values?</p>
        <p>Can we use vectorized sorting algorithms?</p>
        <p>Can we allocate additional memory?</p>
    </section>

    <section class="slide">
        <h2>Integers Sort</h2>

        <p>In ClickHouse for integers and decimals by default we use LSD RadixSort.</p>
        <p>Complexity: <b>O(N * W)</b> where <b>N</b> - number of keys, <b>W</b> - key length. For integers key length
            is constant amount of bytes in the integer type, so complexity is linear.</p>
        <p>It outperforms other sorting algorithms when amount of data is big enough and data does not have any patterns (sorted/almost-sorted/sorted in reverse order).</p>
        <p>Most of the time we sort blocks with 65409 (DEFAULT_BLOCK_SIZE) or 1048449 (DEFAULT_INSERT_BLOCK_SIZE) elements.</p>
        <p>But if data has patterns, for example, already sorted we can use other sorting algorithms.</p>
    </section>

    <section class="slide">
        <h2>Decimals Sort</h2>

        <p>In 2022 when I tried to move decimals sorting to LSD RadixSort, we noticed slow down in some cases when data was already sorted or almost sorted:</p>

        <img style="width: 100%;" src="pictures/decimal_sorting_slowdown.png"/>
    </section>

    <section class="slide">
        <h2>PDQ Sort</h2>

        <p>From Pattern-defeating Quicksort paper:</p>
        <p style="font-size: 16pt;">"The goal of pattern-defeating quicksort (or pdqsort) is to improve on introsort's heuristics
to create a <b style="font-size: 18pt;">hybrid sorting algorithm</b> with several desirable properties. <b style="font-size: 18pt;">It maintains quicksort's logarithmic memory usage
and fast realworld average case, effectively recognizes and combats worst case behavior (deterministically),
and runs in linear time for a few common patterns</b>. It also unavoidably inherits in-place quicksort's instability, so
pdqsort can not be used in situations where stability is needed."</p>
        <p><a href="https://arxiv.org/pdf/2106.05123">Pattern-defeating Quicksort paper</a></p>
        <p><a href="https://github.com/orlp/pdqsort">https://github.com/orlp/pdqsort</a></p>
    </section>

    <section class="slide">
        <h2>Quick Sort</h2>

    <code style="font-size: 12pt;">// Sorts (a portion of) an array, divides it into partitions, then sorts those
algorithm quicksort(A, lo, hi) is
    // Ensure indices are in correct order
    if lo &gt;= hi || lo &lt; 0 then
        return

    // Partition array and get the pivot index
    p := partition(A, lo, hi)

    // Sort the two partitions
    quicksort(A, lo, p - 1) // Left side of pivot
    quicksort(A, p + 1, hi) // Right side of pivot
</code>

    <p style="font-size: 10pt;"></p>

    <p><b>partition</b> function specification - Partition the range: reorder its elements, while determining a point of division, so that all elements with values less than the pivot come
    before the division, while all elements with values greater than the pivot come after it; elements that are equal to the pivot can go either way.</p>

    <p>Sorting the entire array is accomplished by <b>quicksort(A, 0, length(A) - 1)</b>.</p>
    </section>


    <section class="slide">
        <h2>PDQ Sort</h2>

        <p>Basically PDQ Sort is a Quick Sort with a lot of optimizations:</p>
        <p>1. Choose pivot as median of 3 or pseudomedian of 9.</p>
        <p>2. If range was already partitioned attempts to use partial insertion sort that allows
            8 (<b>partial_insertion_sort_limit</b>) amount of element moves before giving up.</p>
        <p>3. Detects highly unbalanced partitions. If partition was highly unbalanced, try to shuffle elements. Fallback to heap sort, if
            there was more than <b>log2(array_size)</b> highly unbalanced partitions.</p>
        <p>4. Tail recursion elimination for the right-hand partition.</p>
        <p>5. For small ranges uses insertion sort.</p>
    </section>

    <section class="slide">
        <h2>PDQSort try_sort</h2>

        <p>We added <b>try_sort</b> function in our fork of pdqsort library and used it in ClickHouse:</p>
        <p><a href="https://github.com/ClickHouse/ClickHouse/pull/35961">https://github.com/ClickHouse/ClickHouse/pull/35961</a></p>

        <code style="font-size: 10pt;">/** Try to fast sort elements for common sorting patterns:
  * 1. If elements are already sorted.
  * 2. If elements are already almost sorted.
  * 3. If elements are already sorted in reverse order.
  *
  * Returns true if fast sort was performed or elements were already sorted, false otherwise.
  */
template &lt;typename RandomIt, typename Compare&gt;
bool trySort(RandomIt first, RandomIt last, Compare compare)
{
#ifndef NDEBUG
    ::shuffle(first, last);
#endif

    ComparatorWrapper&lt;Compare&gt; compare_wrapper = compare;
    return ::pdqsort_try_sort(first, last, compare_wrapper);
}</code>
    </section>

    <section class="slide">
        <h2>PDQSort try_sort</h2>

        <p>In ClickHouse we use this in <b>getPermutation</b> function of <b>ColumnVector</b>. First we try to sort data using <b>pdqsort_try_sort</b>,
            if it is not possible, fallback to LSD RadixSort.</p>

        <code style="font-size: 10pt;">bool try_sort = false;
if (direction == IColumn::PermutationSortDirection::Ascending &&
    stability == IColumn::PermutationSortStability::Unstable)
    try_sort = trySort(res.begin(), res.end(), less(*this, nan_direction_hint));
else if (direction == IColumn::PermutationSortDirection::Ascending &&
    stability == IColumn::PermutationSortStability::Stable)
    try_sort = trySort(res.begin(), res.end(), less_stable(*this, nan_direction_hint));
else if (direction == IColumn::PermutationSortDirection::Descending &&
    stability == IColumn::PermutationSortStability::Unstable)
    try_sort = trySort(res.begin(), res.end(), greater(*this, nan_direction_hint));
else
    try_sort = trySort(res.begin(), res.end(), greater_stable(*this, nan_direction_hint));

if (try_sort)
    return;

PaddedPODArray&lt;ValueWithIndex&lt;T&gt;&gt; pairs(data_size);
for (UInt32 i = 0; i &lt; static_cast&lt;UInt32&gt;(data_size); ++i)
    pairs[i] = {data[i], i};

RadixSort&lt;RadixSortTraits&lt;T&gt;&gt;::executeLSD(pairs.data(), data_size, reverse, res.data());</code>
    </section>

    <section class="slide">
        <h2>Performance improvements</h2>

        <p style="margin-bottom: 2px;">Sorting sorted or sorted in reverse order data. Around 1.5x - 2x performance improvement. Before:</p>

<code style="font-size: 10pt;">SELECT key, value FROM sequential_UInt64 ORDER BY key FORMAT Null
0 rows in set. Elapsed: 1.269 sec. Processed 500.00 million rows, 8.00 GB

SELECT key, value FROM sequential_UInt64 ORDER BY key, value FORMAT Null
0 rows in set. Elapsed: 1.332 sec. Processed 500.00 million rows, 8.00 GB

SELECT key, value FROM sequential_UInt64 ORDER BY key DESC FORMAT Null
0 rows in set. Elapsed: 2.143 sec. Processed 500.00 million rows, 8.00 GB

SELECT key, value FROM sequential_UInt64 ORDER BY key DESC, value DESC FORMAT Null
0 rows in set. Elapsed: 2.438 sec. Processed 500.00 million rows, 8.00 GB</code>

        <p style="margin-bottom: 2px;">After:</p>

<code style="font-size: 10pt;">SELECT key, value FROM sequential_UInt64 ORDER BY key FORMAT Null
0 rows in set. Elapsed: 0.647 sec. Processed 500.00 million rows, 8.00 GB

SELECT key, value FROM sequential_UInt64 ORDER BY key, value FORMAT Null
0 rows in set. Elapsed: 0.657 sec. Processed 500.00 million rows, 8.00 GB

SELECT key, value FROM sequential_UInt64 ORDER BY key DESC FORMAT Null
0 rows in set. Elapsed: 1.480 sec. Processed 500.00 million rows, 8.00 GB

SELECT key, value FROM sequential_UInt64 ORDER BY key DESC, value DESC FORMAT Null
0 rows in set. Elapsed: 1.631 sec. Processed 500.00 million rows, 8.00 GB
</code>
    </section>

    <section class="slide">
        <h2>Performance improvements</h2>

        <p>Additionally performance of INSERT into MergeTree when data is sorted or almost sorted improved by 1.5 - 2x.</p>
    </section>

    <section class="slide">
        <h2>Conclusion</h2>

        <p>Sometimes, you can achieve significant performance improvement not only by using some high-level complex optimizations
            but also by using better algorithms and data structures for your specific tasks or by small source code level optimizations.</p>

        <p>Blog post <a href="https://maksimkita.com/blog/power-of-small-optimizations.html">https://maksimkita.com/blog/power-of-small-optimizations.html</a></p>
        <p>Talk about <a href="https://www.youtube.com/watch?v=timRXe7884I">ClickHouse performance optimization techniques</a> at C++ Russia 2023.</p>
        <p>Talk about <a href="https://www.youtube.com/watch?v=KedCUDZE9N4">ClickHouse performance optimization practices</a> at C++ Russia 2022.</p>
    </section>

    <section class="slide">
        <h2>Questions?</h2>
    </section>

    <div class="progress"></div>
    <script src="shower/shower.min.js"></script>

    <!--Video plugin-->
    <link rel="stylesheet" href="shower/shower-video.css">
    <script src="shower/shower-video.js"></script>
    <!--/Video plugin-->
</body>
</html>
